{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch data\n",
    "\n",
    "PyTorch comes with a nice paradigm for dealing with data which we'll use here. A PyTorch [`Dataset`](http://pytorch.org/docs/master/data.html#torch.utils.data.Dataset) knows where to find data in its raw form (files on disk) and how to load individual examples into Python datastructures. A PyTorch [`DataLoader`](http://pytorch.org/docs/master/data.html#torch.utils.data.DataLoader) takes a dataset and offers a variety of ways to sample batches from that dataset.\n",
    "\n",
    "Take a moment to browse through the `CIFAR10` `Dataset` in `2_pytorch/cifar10.py`, read the `DataLoader` documentation linked above, and see how these are used in the section of `train.py` that loads data. Note that in the first part of the homework we subtracted a mean CIFAR10 image from every image before feeding it in to our models. Here we subtract a constant color instead. Both methods are seen in practice and work equally well.\n",
    "\n",
    "PyTorch provides lots of vision datasets which can be imported directly from [`torchvision.datasets`](http://pytorch.org/docs/master/torchvision/datasets.html). Also see [`torchtext`](https://github.com/pytorch/text#datasets) for natural language datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Classifier in PyTorch\n",
    "\n",
    "In PyTorch Deep Learning building blocks are implemented in the neural network module [`torch.nn`](http://pytorch.org/docs/master/nn.html#) (usually imported as `nn`). A PyTorch model is typically a subclass of [`nn.Module`](http://pytorch.org/docs/master/nn.html#torch.nn.Module) and thereby gains a multitude of features. Now implement a softmax classifier by filling in the marked sections of `models/softmax.py`. Because your logistic regressor is an `nn.Module` all of its parameters and sub-modules are accessible through the `.parameters()` and `.modules()` methods.\n",
    "\n",
    "The next step is to finish the training code begun in `train.py`. This is the main driver. It\n",
    "1. reads arguments and model hyperparameter from the command line\n",
    "2. loads CIFAR10 data\n",
    "3. loads the specified model (in this case, softmax)\n",
    "4. trains the model\n",
    "5. reports performance on test data\n",
    "\n",
    "At this point all of these steps are complete for the softmax classifier except step 4. Finish implementing the training loop in `train.py` using PyTorch components then run\n",
    "\n",
    "    $ run_softmax.sh\n",
    "\n",
    "to train a model and save it to `softmax.pt`. This will also produce a `softmax.log` file which contains training details which we will visualize below. You may want to adjust the hyperparameters specified in `run_softmax.sh` to get reasonable performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the PyTorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Assuming that you have completed training the classifer, let us plot the training loss vs. iteration. This is an\n",
    "# example to show a simple way to log and plot data from PyTorch.\n",
    "\n",
    "# we need matplotlib to plot the graphs for us!\n",
    "import matplotlib\n",
    "# This is needed to save images \n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Parse the train and val losses one line at a time.\n",
    "import re\n",
    "# regexes to find train and val losses on a line\n",
    "float_regex = r'[-+]?(\\d+(\\.\\d*)?|\\.\\d+)([eE][-+]?\\d+)?'\n",
    "train_loss_re = re.compile('.*Train Loss: ({})'.format(float_regex))\n",
    "val_loss_re = re.compile('.*Val Loss: ({})'.format(float_regex))\n",
    "val_acc_re = re.compile('.*Val Acc: ({})'.format(float_regex))\n",
    "# extract one loss for each logged iteration\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_accs = []\n",
    "# NOTE: You may need to change this file name.\n",
    "with open('softmax.log', 'r') as f:\n",
    "    for line in f:\n",
    "        train_match = train_loss_re.match(line)\n",
    "        val_match = val_loss_re.match(line)\n",
    "        val_acc_match = val_acc_re.match(line)\n",
    "        if train_match:\n",
    "            train_losses.append(float(train_match.group(1)))\n",
    "        if val_match:\n",
    "            val_losses.append(float(val_match.group(1)))\n",
    "        if val_acc_match:\n",
    "            val_accs.append(float(val_acc_match.group(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(train_losses, label='train')\n",
    "plt.plot(val_losses, label='val')\n",
    "plt.title('Softmax Learning Curve')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(val_accs, label='val')\n",
    "plt.title('Softmax Validation Accuracy During Training')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
